{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJiO2375pF4x"
      },
      "source": [
        "# Romuald Ricard 261194253\n",
        "# Faiyad Irfan Hares 260914739"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdWS6piawQCj",
        "outputId": "c12afd31-0ffe-4851-d021-0e40e50de90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#PATH = \"/content/drive/MyDrive/ECSE415/\"\n",
        "PATH = \"/content/drive/MyDrive/McGill/ECSE415 FINAL PROJECT RICARD HARES/\"\n",
        "\n",
        "pathmcgill = PATH + \"mcgill_drive.mp4\"\n",
        "pathstcat = '/content/drive/MyDrive/ECSE415/st-catherines_drive.mp4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPFLpq8YR10R"
      },
      "source": [
        "# TRACKING WITH BOUNDING-BOXES OVERLAP AREA AND OPTICAL FLOW\n",
        "The idea here is to get a metric of how likely a detected bounding-box corresponds to a certain tracked object.\n",
        "\n",
        "We use the overlaping area ratio for detection. By computing the Intersection Area Matrix (IAM) for each tracked-bb and detected-bb (which taks the following form).\n",
        "\n",
        "A tracked object has a \"confidence score\" from 0 to 1. Once its reached a threshold (0.5), it is counted as a valid tracked object and added to the count. We increment the \"confidence score\" every time the object has been successfully tracked, and decrement if not. Once its reached 0, we end it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AmBx4v7bJJZ"
      },
      "outputs": [],
      "source": [
        "class RectangleIntersectionTracker:\n",
        "\n",
        "  def __init__(self, bb, label):\n",
        "\n",
        "    # Initialize boundingbox\n",
        "    self.bb = bb\n",
        "\n",
        "    # Initialize confidence score and confirm flag\n",
        "    self.confidence = 0\n",
        "    self.confidence_rate = 0.1\n",
        "    self.confirmed = False\n",
        "    self.confidence_confirm_threshold = 0.5\n",
        "\n",
        "    # Initialize parked/moving car\n",
        "    self.RV = 0\n",
        "    self.RV_threshold = 0.25\n",
        "\n",
        "    self.moving_score = 0\n",
        "    self.moving_score_rate = 0.1\n",
        "    self.moving_score_threshold = 1.5\n",
        "\n",
        "    self.parked = None\n",
        "\n",
        "    # Tracker label & color\n",
        "    self.label = label\n",
        "    if label == \"car\":\n",
        "      self.col = [0, 0, 1]\n",
        "    elif label == \"person\":\n",
        "      self.col = [1, 0, 0]\n",
        "    else:\n",
        "      self.col = [0, 0, 0]\n",
        "\n",
        "  def update(self, bb):\n",
        "\n",
        "    # Update bb\n",
        "    self.bb = bb\n",
        "\n",
        "    # Update confidence\n",
        "    self.confidence = min(1, self.confidence + self.confidence_rate)\n",
        "    self.check_confirm()\n",
        "\n",
        "    # Update parked flag\n",
        "    if self.label == \"car\":\n",
        "      self.check_parked()\n",
        "\n",
        "  def check_confirm(self):\n",
        "    if not self.confirmed and self.confidence > self.confidence_confirm_threshold:\n",
        "\n",
        "      # Confirm flag\n",
        "      self.confirmed = True\n",
        "\n",
        "      # Assign id and update global count\n",
        "      if self.label == \"car\":\n",
        "        global CARS\n",
        "        CARS += 1\n",
        "        self.id = CARS\n",
        "\n",
        "      elif self.label == \"person\":\n",
        "        global PEDESTRIANS\n",
        "        PEDESTRIANS += 1\n",
        "        self.id = PEDESTRIANS\n",
        "\n",
        "  def check_parked(self):\n",
        "\n",
        "    # Update parked confidence\n",
        "    if self.RV < self.RV_threshold:\n",
        "      self.moving_score += self.moving_score_rate\n",
        "    else:\n",
        "      self.moving_score -= self.moving_score_rate\n",
        "\n",
        "    # Check if parked\n",
        "    if self.parked is None and self.confirmed:\n",
        "      if self.moving_score > self.moving_score_threshold:\n",
        "        self.col = [0, 1, 0]\n",
        "        self.parked = True\n",
        "        global PARKED_CARS\n",
        "        PARKED_CARS += 1\n",
        "\n",
        "      if self.moving_score < - self.moving_score_threshold:\n",
        "        self.col = [0, 0, 1]\n",
        "        self.parked = False\n",
        "\n",
        "  def update_no_measure(self):\n",
        "    self.confidence = self.confidence - self.confidence_rate\n",
        "\n",
        "  def is_dead(self):\n",
        "    return(self.confidence < 0)\n",
        "\n",
        "  def draw_on_mask(self, mask):\n",
        "\n",
        "    if self.confirmed:\n",
        "      # Color\n",
        "      col = [int(255*self.confidence)*c for c in self.col]\n",
        "\n",
        "      # Draw rectangle\n",
        "      x1 = int(self.bb[0])\n",
        "      y1 = int(self.bb[1])\n",
        "      x2 = int(self.bb[2])\n",
        "      y2 = int(self.bb[3])\n",
        "      mask = cv2.rectangle(mask, [x1, y1], [x2, y2], col, 1)\n",
        "\n",
        "      # Dislay id\n",
        "      mask = cv2.putText(mask, self.label + \" \" + str(self.id), [x1, y1], cv2.FONT_HERSHEY_SIMPLEX, 0.4, col, 1, cv2.LINE_AA)\n",
        "\n",
        "      # Dipslay RV\n",
        "      mask = cv2.putText(mask, str(round(self.RV, 2)), [int((x1+x2)/2), y1], cv2.FONT_HERSHEY_SIMPLEX, 0.4, col, 1, cv2.LINE_AA)\n",
        "\n",
        "    return(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anp9NdOLhAbi"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# COMPUTES THE INTERSECTION AREA OF 2 RECTANGLES\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_rectangle_intersection_area(bb1, bb2):\n",
        "  \"\"\"\n",
        "  Bounding Boxes (bb) have to be of format xyxy.\n",
        "  The intersection area is expressed as a ratio of bb1 area and intersection area.\n",
        "  \"\"\"\n",
        "\n",
        "  # Area of bb1\n",
        "  a = abs(bb1[2] - bb1[0])*abs(bb1[3] - bb1[1])\n",
        "\n",
        "  # Compute intersection sides\n",
        "  dx = min(bb1[2], bb2[2]) - max(bb1[0], bb2[0])\n",
        "  dy = min(bb1[3], bb2[3]) - max(bb1[1], bb2[1])\n",
        "\n",
        "  # If intersection\n",
        "  if dx >= 0 and dy >= 0:\n",
        "    return(dx*dy/a)\n",
        "  else:\n",
        "    return(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# RETURNS ALL LABELED BOUNDING-BOXES FOUND IN FRAME\n",
        "# WITH CONFIDENCE GREATER THAN THRESHOLD\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def detect_bbox_label(frame, label, model, threshold = 0.3):\n",
        "  \"\"\"\n",
        "  INPUTS:\n",
        "    - frame: cv2 image of the current frame\n",
        "    - label: label to be detected (\"car\", \"person\"...)\n",
        "    - model: classifier\n",
        "  OUTPUT:\n",
        "  \"\"\"\n",
        "\n",
        "  # Run model\n",
        "  res = model(frame)\n",
        "  df = res.pandas().xyxy[0]\n",
        "\n",
        "  # Get cars detected bb\n",
        "  bbox_list = []\n",
        "  for idx in df.index:\n",
        "    if df[\"name\"][idx] == label and df[\"confidence\"][idx] > threshold:\n",
        "      bbox = [df[\"xmin\"][idx], df[\"ymin\"][idx], df[\"xmax\"][idx], df[\"ymax\"][idx]]\n",
        "      bbox_list.append(bbox)\n",
        "  return(bbox_list)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# DIVIDES TRACKERS AND BOUNDING-BOXES INTO 3 GROUPS (SEE REPORT)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def match_trackers_bboxes(trackers, bboxes, threshold = 0.5):\n",
        "\n",
        "  # Compute intersection area matrix\n",
        "  n = len(trackers)\n",
        "  m = len(bboxes)\n",
        "\n",
        "  # List of un-matched trackers/bboxes\n",
        "  unmatched_trackers_idx = [i for i in range(n)]\n",
        "  unmatched_bboxes_idx = [j for j in range(m)]\n",
        "  matched_idx = []\n",
        "\n",
        "  if n > 0 and m > 0:\n",
        "    IAM = np.zeros([n, m])\n",
        "    for i in range(n):\n",
        "      t = trackers[i]\n",
        "      for j in range(m):\n",
        "        bb = bboxes[j]\n",
        "        IAM[i, j] = compute_rectangle_intersection_area(t.bb, bb)\n",
        "\n",
        "    success = True\n",
        "    while success and IAM.shape[0]*IAM.shape[1] != 0:\n",
        "      [i_max, j_max] = np.unravel_index(IAM.argmax(), IAM.shape)\n",
        "      if IAM[i_max, j_max] > threshold:\n",
        "\n",
        "        # Add (t_idx, b_idx) to matches\n",
        "        matched_idx.append([unmatched_trackers_idx[i_max], unmatched_bboxes_idx[j_max]])\n",
        "\n",
        "        # Remove t_idx and bb_idx from un-matched lists\n",
        "        unmatched_trackers_idx = np.delete(unmatched_trackers_idx, i_max)\n",
        "        unmatched_bboxes_idx = np.delete(unmatched_bboxes_idx, j_max)\n",
        "\n",
        "        # Remove matched line and col. of IAM\n",
        "        IAM = np.delete(IAM, i_max, axis = 0)\n",
        "        IAM = np.delete(IAM, j_max, axis = 1)\n",
        "      else:\n",
        "        success = False\n",
        "\n",
        "  # Output\n",
        "  matches = [(trackers[m[0]], bboxes[m[1]]) for m in matched_idx]\n",
        "  unmatched_trackers = [trackers[i] for i in unmatched_trackers_idx]\n",
        "  unmatched_bboxes = [bboxes[j] for j in unmatched_bboxes_idx]\n",
        "  return(matches, unmatched_trackers, unmatched_bboxes)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# COMPUTES GLOBAL VELOCITY AND TRACKERS RELATIVE VELOCITIES TO THE SCENE\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def velocities_in_scene(current_frame, prev_frame, trackers):\n",
        "\n",
        "    # Convert frames to grayscale\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate dense optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Compute magnitude and angle of the flow vectors across whole frame\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Scene average velocity\n",
        "    scene_velocity = np.mean(magnitude)\n",
        "\n",
        "    # Get average magnitude in bb of trackers\n",
        "    tracker_velocities = []\n",
        "    for t in trackers:\n",
        "      bb = t.bb\n",
        "      x1 = int(bb[0])\n",
        "      y1 = int(bb[1])\n",
        "      x2 = int(bb[2])\n",
        "      y2 = int(bb[3])\n",
        "\n",
        "      # Compute average velocity in bb\n",
        "      magnitude_in_bb = magnitude[x1:x2, y1:y2]\n",
        "      s = magnitude_in_bb.shape\n",
        "      if s[0]*s[1] > 0:\n",
        "        tracker_relative_velocity = np.mean(magnitude_in_bb)/scene_velocity\n",
        "        t.RV = tracker_relative_velocity\n",
        "\n",
        "    return(scene_velocity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyse_video(model, cap, QUALITY = 30):\n",
        "\n",
        "  # Read first frame\n",
        "  success, prev_frame = cap.read()\n",
        "\n",
        "  # Scale factor to reduce frame quality, resize first frame\n",
        "  width = int(prev_frame.shape[1] * QUALITY/100)\n",
        "  height = int(prev_frame.shape[0] * QUALITY/100)\n",
        "  dim = (width, height)\n",
        "  prev_frame = cv2.resize(prev_frame, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "  # Initialize count of labels\n",
        "  global AVERAGE_SCENE_SPEED\n",
        "  AVERAGE_SCENE_SPEED = 0\n",
        "  global CARS\n",
        "  CARS = 0\n",
        "  global PARKED_CARS\n",
        "  PARKED_CARS = 0\n",
        "  global PEDESTRIANS\n",
        "  PEDESTRIANS = 0\n",
        "\n",
        "  # Initialize tracker list\n",
        "  car_trackers = []\n",
        "  pedestrian_trackers = []\n",
        "\n",
        "  # Create output video with new FPS and dimensions\n",
        "  vid = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'MJPG'), 30, dim)\n",
        "\n",
        "  TOTAL_FRAMES = 1399\n",
        "  for N in tqdm(range(TOTAL_FRAMES)):\n",
        "\n",
        "    # Read frame\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "\n",
        "      # Reduce size of image\n",
        "      frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "      # Compute optical flow to estimate velocity of scene, update trackers rv\n",
        "      AVERAGE_SCENE_SPEED = velocities_in_scene(frame, prev_frame, car_trackers)\n",
        "\n",
        "      # Detect cars & pedestrians\n",
        "      car_bboxes = detect_bbox_label(frame, \"car\", model, threshold = 0.5)\n",
        "      pedestrian_bboxes = detect_bbox_label(frame, \"person\", model, threshold = 0.5)\n",
        "\n",
        "      # Find matches, un-matched trackers and un-matches bboxes\n",
        "      [match_cars, unmatched_car_trackers, unmatched_car_bboxes] = match_trackers_bboxes(car_trackers, car_bboxes)\n",
        "      [match_pedestrians, unmatched_pedestrian_trackers, unmatched_pedestrian_bboxes] = match_trackers_bboxes(pedestrian_trackers, pedestrian_bboxes)\n",
        "\n",
        "      # Update matched trackers\n",
        "      for m in match_cars + match_pedestrians:\n",
        "        m[0].update(m[1])\n",
        "\n",
        "      # Update un-matched trackers\n",
        "      for t in unmatched_car_trackers + unmatched_pedestrian_trackers:\n",
        "        t.update_no_measure()\n",
        "\n",
        "      # Remove \"dead\" trackers\n",
        "      for t in car_trackers:\n",
        "        if t.is_dead():\n",
        "          car_trackers.remove(t)\n",
        "\n",
        "      for t in pedestrian_trackers:\n",
        "        if t.is_dead():\n",
        "          pedestrian_trackers.remove(t)\n",
        "\n",
        "      # Create trackers for un-matched bboxes\n",
        "      for bb in unmatched_car_bboxes:\n",
        "        car_trackers.append(RectangleIntersectionTracker(bb, \"car\"))\n",
        "      for bb in unmatched_pedestrian_bboxes:\n",
        "        pedestrian_trackers.append(RectangleIntersectionTracker(bb, \"person\"))\n",
        "\n",
        "      # Update previous frame\n",
        "      prev_frame = frame\n",
        "\n",
        "      # Add info to frame\n",
        "      mask = np.zeros_like(frame)\n",
        "      for t in car_trackers + pedestrian_trackers:\n",
        "        mask = t.draw_on_mask(mask)\n",
        "      img = cv2.add(frame, mask)\n",
        "\n",
        "      # Display and record\n",
        "      vid.write(img)\n",
        "      #cv2_imshow(img)\n",
        "\n",
        "  cap.release()\n",
        "  print(\"MOVING CARS \" + str(CARS - PARKED_CARS) + \" | PARKED CARS \" + str(PARKED_CARS) + \" | PEDESTRIANS \" + str(PEDESTRIANS))"
      ],
      "metadata": {
        "id": "Yy92PoEtET2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create object detection model (YOLOv5s)\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", _verbose = False)\n",
        "\n",
        "# Open video\n",
        "cap1 = cv2.VideoCapture(PATH + \"st-catherines_drive.mp4\")\n",
        "cap2 = cv2.VideoCapture(PATH + \"mcgill_drive.mp4\")\n",
        "\n",
        "# Analyse video\n",
        "analyse_video(model, cap1)\n",
        "analyse_video(model, cap2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "NZW-tVAvEscy",
        "outputId": "95f85559-aa7f-427c-86cd-09ec78af9049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            " 79%|███████▉  | 1107/1399 [13:24<03:32,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-71732e566d02>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Analyse video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manalyse_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0manalyse_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c45b83418d3e>\u001b[0m in \u001b[0;36manalyse_video\u001b[0;34m(model, cap, QUALITY)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# Reduce size of image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# Compute optical flow to estimate velocity of scene, update trackers rv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L040uzXkywMY"
      },
      "source": [
        "# Alternative Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GidntjNByzA_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the centroid of a bounding box\n",
        "def get_centroid(xmin, ymin, xmax, ymax):\n",
        "    return (xmin + xmax) / 2, (ymin + ymax) / 2\n",
        "\n",
        "# Function to find the closest tracked object\n",
        "def find_closest_tracked(centroid, tracked_objects, distance_threshold=95):\n",
        "    closest_distance = float('inf')\n",
        "    closest_id = None\n",
        "    for obj_id, obj_data in tracked_objects.items():\n",
        "        distance = np.linalg.norm(np.array(obj_data['centroid']) - np.array(centroid))\n",
        "        if distance < closest_distance and distance < distance_threshold:\n",
        "            closest_distance = distance\n",
        "            closest_id = obj_id\n",
        "    return closest_id\n",
        "\n",
        "# Function to check if the centroid is near the edge of the frame\n",
        "def is_near_edge(centroid, frame_width, frame_height, edge_threshold=10000):\n",
        "    cx, cy = centroid\n",
        "    return cx < edge_threshold or cx > frame_width - edge_threshold or \\\n",
        "           cy < edge_threshold or cy > frame_height - edge_threshold\n",
        "\n",
        "def process_cars(model, video_path, movement_threshold):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    parked_car_count = 0\n",
        "    moving_car_count = 0\n",
        "    tracked_objects = {}\n",
        "    next_id = 1\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    movement_threshold = movement_threshold # Define a suitable threshold for movement\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform detection using YOLOv5\n",
        "        results = model(frame)\n",
        "\n",
        "        # Update centroids for tracked objects\n",
        "        for obj_id in list(tracked_objects):\n",
        "            tracked_objects[obj_id]['previous_centroid'] = tracked_objects[obj_id]['centroid']\n",
        "\n",
        "        # Process detections for cars\n",
        "        for *xyxy, conf, cls in results.xyxy[0]:\n",
        "            label = model.names[int(cls)]\n",
        "            if label == 'car':\n",
        "                xmin, ymin, xmax, ymax = map(int, xyxy)\n",
        "                centroid = get_centroid(xmin, ymin, xmax, ymax)\n",
        "\n",
        "                if is_near_edge(centroid, frame_width, frame_height):\n",
        "                    closest_id = find_closest_tracked(centroid, tracked_objects)\n",
        "                    if closest_id is None:\n",
        "                        tracked_objects[next_id] = {'centroid': centroid, 'previous_centroid': centroid, 'label': 'car'}\n",
        "                        next_id += 1\n",
        "                    else:\n",
        "                        # Check if the car has moved\n",
        "                        prev_centroid = tracked_objects[closest_id]['previous_centroid']\n",
        "                        distance_moved = np.linalg.norm(np.array(prev_centroid) - np.array(centroid))\n",
        "                        if distance_moved > movement_threshold:\n",
        "                            # Car is moving\n",
        "                            if 'counted' not in tracked_objects[closest_id] or not tracked_objects[closest_id]['counted']:\n",
        "                                moving_car_count += 1\n",
        "                                tracked_objects[closest_id]['counted'] = True\n",
        "                        else:\n",
        "                            # Car is parked\n",
        "                            if 'counted' not in tracked_objects[closest_id] or not tracked_objects[closest_id]['counted']:\n",
        "                                parked_car_count += 1\n",
        "                                tracked_objects[closest_id]['counted'] = True\n",
        "\n",
        "    cap.release()\n",
        "    return parked_car_count, moving_car_count\n",
        "\n",
        "# Function to process video for pedestrians\n",
        "def process_pedestrians(model, video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    pedestrian_count = 0\n",
        "    tracked_objects = {}\n",
        "    next_id = 1\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform detection using YOLOv5\n",
        "        results = model(frame)\n",
        "\n",
        "        # Process detections for pedestrians\n",
        "        for *xyxy, conf, cls in results.xyxy[0]:\n",
        "            label = model.names[int(cls)]\n",
        "            if label == 'person':\n",
        "                xmin, ymin, xmax, ymax = map(int, xyxy)\n",
        "                centroid = get_centroid(xmin, ymin, xmax, ymax)\n",
        "\n",
        "                if is_near_edge(centroid, frame_width, frame_height):\n",
        "                    closest_id = find_closest_tracked(centroid, tracked_objects)\n",
        "                    if closest_id is None:\n",
        "                        tracked_objects[next_id] = {'centroid': centroid, 'label': 'person'}\n",
        "                        pedestrian_count += 1\n",
        "                        next_id += 1\n",
        "\n",
        "    cap.release()\n",
        "    return pedestrian_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E9jnyXdWzHbf",
        "outputId": "211bfde8-f389-4823-ff26-4c32ad4f7d84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2023-12-6 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parked cars mcgill_drive.mp4: 11\n",
            "Moving cars mcgill_drive.mp4: 21\n",
            "Total pedestrians mcgill_drive.mp4: 30\n",
            "Parked cars st-catherines_drive.mp4: 45\n",
            "Moving cars st-catherines_drive.mp4: 0\n",
            "Total pedestrians st-catherines_drive.mp4: 55\n"
          ]
        }
      ],
      "source": [
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "#process\n",
        "parkedMcgill, movingMcGill=process_cars(model, pathmcgill, 10)\n",
        "pedestrianMcgill = process_pedestrians(model, pathmcgill)\n",
        "\n",
        "\n",
        "parkedStcat, movingStCat=process_cars(model, pathstcat, 88)\n",
        "pedestrianStcat = process_pedestrians(model, pathstcat)\n",
        "\n",
        "print(f\"Parked cars mcgill_drive.mp4: {parkedMcgill}\")\n",
        "print(f\"Moving cars mcgill_drive.mp4: {movingMcGill}\")\n",
        "print(f\"Total pedestrians mcgill_drive.mp4: {pedestrianMcgill}\")\n",
        "\n",
        "print(f\"Parked cars st-catherines_drive.mp4: {parkedStcat}\")\n",
        "print(f\"Moving cars st-catherines_drive.mp4: {movingStCat}\")\n",
        "print(f\"Total pedestrians st-catherines_drive.mp4: {pedestrianStcat}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}